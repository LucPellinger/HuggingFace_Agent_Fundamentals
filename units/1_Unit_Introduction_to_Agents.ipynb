{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f98ea371",
   "metadata": {},
   "source": [
    "# 1. Introduction to Agents\n",
    "\n",
    "## Objective\n",
    "\n",
    "- Understanding Agents\n",
    "    - What is an Agent, and how does it work?\n",
    "    - How do Agents make decisions using reasoning and planning?\n",
    "\n",
    "- The Role of LLMs (Large Language Models) in Agents\n",
    "\n",
    "    - How LLMs serve as the ‚Äúbrain‚Äù behind an Agent.\n",
    "    - How LLMs structure conversations via the Messages system.\n",
    "\n",
    "- Tools and Actions\n",
    "    - How Agents use external tools to interact with the environment.\n",
    "    - How to build and integrate tools for your Agent.\n",
    "    \n",
    "- The Agent Workflow:\n",
    "    - Think ‚Üí Act ‚Üí Observe.\n",
    "\n",
    "\n",
    "After exploring these topics, **you‚Äôll build your first Agent** using smolagents!\n",
    "\n",
    "Your Agent, named Alfred, will handle a simple task and demonstrate how to apply these concepts in practice.\n",
    "\n",
    "You‚Äôll even learn how to **publish your Agent on Hugging Face Spaces**, so you can share it with friends and colleagues.\n",
    "\n",
    "Finally, at the end of this Unit, you‚Äôll take a quiz. Pass it, and you‚Äôll **earn your first course certification**: the üéì Certificate of Fundamentals of Agents.\n",
    "\n",
    "<img src=\"../assets/cetification_fundamentals.png\" alt=\"Certification Fundamentals\" width=\"400px\" />\n",
    "\n",
    "\n",
    "## Unit Outline\n",
    "\n",
    "This Unit is your essential starting point, laying the groundwork for understanding Agents before you move on to more advanced topics.\n",
    "\n",
    "<img src=\"../assets/Unit1_outline.png\" alt=\"Unit 1 Outline\" width=\"600px\" />\n",
    "\n",
    "\n",
    "### Step 1: What is an Agent?\n",
    "\n",
    "An Agent is a system that can perceive its environment, reason about it, and take actions to achieve specific goals.\n",
    "\n",
    "- Agents understand natural language and can interact with users in a conversational manner.\n",
    "- They can also use external tools to perform tasks, such as searching the web, accessing databases, or executing code.\n",
    "- Agents can learn from their experiences and improve their performance over time.\n",
    "- They can also adapt to new situations and environments, making them versatile and flexible.\n",
    "\n",
    "**Agents working logic:**\n",
    "\n",
    "Agents fullfill a request or task by following a simple workflow:\n",
    "1. **Think**: The Agent uses *reasoning and planning* to understand the task and decide on the best course of action.\n",
    "2. **Act**: The Agent takes action by using external tools or APIs to perform the task. For this he is usually provided with a set of tools he should know about and how to use them.\n",
    "3. **Observe**: The Agent observes the results of its actions and uses this feedback to improve its performance in the future.\n",
    "\n",
    "#### **Standard Definition:**\n",
    "\n",
    "*Short*\n",
    "\n",
    "what an Agent is: **an AI model capable of reasoning, planning, and interacting with its environment.**\n",
    "\n",
    "*Long*\n",
    "\n",
    "An Agent is a system that **can perceive its environment**, **reason** about it, and **take actions** to achieve specific goals. It uses **reasoning and planning** to understand tasks, takes **action using external tools**, and **learns from its experiences** to improve performance over time. \n",
    "\n",
    "It is called Agent because it has agency, aka it has the ability to interact with the environment.\n",
    "\n",
    "<img src=\"../assets/Unit1_step1_Agent_Outline.png.png\" alt=\"Agent\" width=\"700px\" />\n",
    "\n",
    "\n",
    "#### Formal Definition AI Agent\n",
    "\n",
    "\"An Agent is a system that leverages an AI model to interact with its environment in order to achieve a user-defined objective. It combines reasoning, planning, and the execution of actions (often via external tools) to fulfill tasks.\"\n",
    "\n",
    "The Agent has two parts:\n",
    "\n",
    "- AI model as its brain\n",
    "- Environment as its body (capabilities and tools)\n",
    "\n",
    "#### Spectrum of \"Agency\"\n",
    "\n",
    "| Agency Level | Description                                               | What that‚Äôs called   | Example pattern                                                 |\n",
    "|--------------|-----------------------------------------------------------|-----------------------|------------------------------------------------------------------|\n",
    "| ‚òÜ‚òÜ‚òÜ          | Agent output has no impact on program flow                | Simple processor      | `process_llm_output(llm_response)`                              |\n",
    "| ‚òÖ‚òÜ‚òÜ          | Agent output determines basic control flow                | Router                | `if llm_decision(): path_a() else: path_b()`                    |\n",
    "| ‚òÖ‚òÖ‚òÜ          | Agent output determines function execution                | Tool caller           | `run_function(llm_chosen_tool, llm_chosen_args)`                |\n",
    "| ‚òÖ‚òÖ‚òÖ          | Agent output controls iteration and program continuation  | Multi-step Agent      | `while llm_should_continue(): execute_next_step()`              |\n",
    "| ‚òÖ‚òÖ‚òÖ          | One agentic workflow can start another agentic workflow   | Multi-Agent           | `if llm_trigger(): execute_agent()`                             |\n",
    "\n",
    "\n",
    "\n",
    "#### What type of AI Models do we use for Agents?\n",
    "\n",
    "The most common AI model found in Agents is an LLM (Large Language Model), which takes Text as an input and outputs Text as well.\n",
    "\n",
    "Well known examples are GPT4 from OpenAI, LLama from Meta, Gemini from Google, etc. These models have been trained on a vast amount of text and are able to generalize well. We will learn more about LLMs in the next section.\n",
    "\n",
    "        **It's also possible to use models that accept other inputs as the Agent's core model. For example, a Vision Language Model (VLM), which is like an LLM but also understands images as input. We'll focus on LLMs for now and will discuss other options later.**\n",
    "\n",
    "#### How does an AI take action on its environment?\n",
    "\n",
    "LLMs are amazing models, but they can only generate text.\n",
    "\n",
    "However, if you ask a well-known chat application like HuggingChat or ChatGPT to generate an image, they can! How is that possible?\n",
    "\n",
    "The answer is that the developers of HuggingChat, ChatGPT and similar apps implemented additional functionality (called Tools), that the LLM can use to create images.\n",
    "\n",
    "\n",
    "#### What type of tasks can an Agent do?\n",
    "\n",
    "An Agent can perform any task we implement via Tools to complete Actions.\n",
    "\n",
    "For example, if I write an Agent to act as my personal assistant (like Siri) on my computer, and I ask it to ‚Äúsend an email to my Manager asking to delay today‚Äôs meeting‚Äù, I can give it some code to send emails. This will be a new Tool the Agent can use whenever it needs to send an email. We can write it in Python:\n",
    "\n",
    "   ```python\n",
    "   def send_message_to(recipient, message):\n",
    "       \"\"\"Useful to send an e-mail message to a recipient\"\"\"\n",
    "       pass\n",
    "   ```\n",
    "\n",
    "The LLM, as we‚Äôll see, will generate code to run the tool when it needs to, and thus fulfill the desired task.\n",
    "\n",
    "   ```python\n",
    "   send_message_to(\"Manager\", \"Can we postpone today's meeting?\")\n",
    "   ```\n",
    "\n",
    "The design of the Tools is very important and has a great impact on the quality of your Agent. Some tasks will require very specific Tools to be crafted, while others may be solved with general purpose tools like ‚Äúweb_search‚Äù.\n",
    "\n",
    "        \"Note that Actions are not the same as Tools. An Action, for instance, can involve the use of multiple Tools to complete.\"\n",
    "\n",
    "Allowing an agent to interact with its environment allows real-life usage for companies and individuals.\n",
    "\n",
    "\n",
    "##### Example 1: Personal Virtual Assistants\n",
    "\n",
    "Virtual assistants like Siri, Alexa, or Google Assistant, work as agents when they interact on behalf of users using their digital environments.\n",
    "\n",
    "They take user queries, analyze context, retrieve information from databases, and provide responses or initiate actions (like setting reminders, sending messages, or controlling smart devices).\n",
    "\n",
    "##### Example 2: Customer Service Chatbots\n",
    "\n",
    "Many companies deploy chatbots as agents that interact with customers in natural language.\n",
    "\n",
    "These agents can answer questions, guide users through troubleshooting steps, open issues in internal databases, or even complete transactions.\n",
    "\n",
    "Their predefined objectives might include improving user satisfaction, reducing wait times, or increasing sales conversion rates. By interacting directly with customers, learning from the dialogues, and adapting their responses over time, they demonstrate the core principles of an agent in action.\n",
    "\n",
    "##### Example 3: AI Non-Playable Character in a video game\n",
    "\n",
    "AI agents powered by LLMs can make Non-Playable Characters (NPCs) more dynamic and unpredictable.\n",
    "\n",
    "Instead of following rigid behavior trees, they can respond contextually, adapt to player interactions, and generate more nuanced dialogue. This flexibility helps create more lifelike, engaging characters that evolve alongside the player‚Äôs actions.\n",
    "\n",
    "\n",
    "#### Summary:\n",
    "\n",
    "To summarize, an Agent is a system that uses an AI Model (typically an LLM) as its core reasoning engine, to:\n",
    "\n",
    "- Understand natural language: Interpret and respond to human instructions in a meaningful way.\n",
    "\n",
    "- Reason and plan: Analyze information, make decisions, and devise strategies to solve problems.\n",
    "\n",
    "- Interact with its environment: Gather information, take actions, and observe the results of those actions.\n",
    "\n",
    "Now that you have a solid grasp of what Agents are, let‚Äôs reinforce your understanding with a short, ungraded quiz. After that, we‚Äôll dive into the ‚ÄúAgent‚Äôs brain‚Äù: the LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09730f89",
   "metadata": {},
   "source": [
    "### Step 2: What are LLMs?\n",
    "\n",
    "<img src=\"../assets/Unit1_step2.png\" alt=\"LLM\" width=\"700px\" />\n",
    "\n",
    "In the previous section we learned that each Agent needs an AI Model at its core, and that LLMs are the most common type of AI models for this purpose.\n",
    "\n",
    "Now we will learn what LLMs are and how they power Agents.\n",
    "\n",
    "This section offers a concise technical explanation of the use of LLMs. If you want to dive deeper, you can check our free Natural Language Processing Course.\n",
    "\n",
    "\n",
    "#### What is a Large Language Model?\n",
    "\n",
    "An LLM is a type of AI model that excels at understanding and generating human language. They are trained on vast amounts of text data, allowing them to learn patterns, structure, and even nuance in language. These models typically consist of many millions of parameters.\n",
    "\n",
    "Most LLMs nowadays are built on the Transformer architecture‚Äîa deep learning architecture based on the ‚ÄúAttention‚Äù algorithm, that has gained significant interest since the release of BERT from Google in 2018.\n",
    "\n",
    "There are 3 types of transformers:\n",
    "\n",
    "1. Encoders\n",
    "\n",
    "    An encoder-based Transformer takes text (or other data) as input and outputs a dense representation (or embedding) of that text.\n",
    "\n",
    "    - Example: BERT from Google\n",
    "    - Use Cases: Text classification, semantic search, Named Entity Recognition\n",
    "    - Typical Size: Millions of parameters\n",
    "\n",
    "2. Decoders\n",
    "\n",
    "    A decoder-based Transformer focuses on generating new tokens to complete a sequence, one token at a time.\n",
    "\n",
    "    - Example: Llama from Meta\n",
    "    - Use Cases: Text generation, chatbots, code generation\n",
    "    - Typical Size: Billions (in the US sense, i.e., 10^9) of parameters\n",
    "\n",
    "3. Seq2Seq (Encoder‚ÄìDecoder)\n",
    "\n",
    "    A sequence-to-sequence Transformer combines an encoder and a decoder. The encoder first processes the input sequence into a context representation, then the decoder generates an output sequence.\n",
    "\n",
    "    - Example: T5, BART\n",
    "    - Use Cases: Translation, Summarization, Paraphrasing\n",
    "    - Typical Size: Millions of parameters\n",
    "\n",
    "Although Large Language Models come in various forms, LLMs are typically decoder-based models with billions of parameters. Here are some of the most well-known LLMs:\n",
    "\n",
    "\n",
    "| Model        | Provider                    |\n",
    "|--------------|-----------------------------|\n",
    "| Deepseek-R1  | DeepSeek                    |\n",
    "| GPT4         | OpenAI                      |\n",
    "| Llama 3      | Meta (Facebook AI Research) |\n",
    "| SmolLM2      | Hugging Face                |\n",
    "| Gemma        | Google                      |\n",
    "| Mistral      | Mistral                     |\n",
    "\n",
    "\n",
    "The underlying principle of an LLM is simple yet highly effective: its objective is to predict the next token, given a sequence of previous tokens. A ‚Äútoken‚Äù is the unit of information an LLM works with. You can think of a ‚Äútoken‚Äù as if it was a ‚Äúword‚Äù, but for efficiency reasons LLMs don‚Äôt use whole words.\n",
    "\n",
    "For example, while English has an estimated 600,000 words, an LLM might have a vocabulary of around 32,000 tokens (as is the case with Llama 2). Tokenization often works on sub-word units that can be combined.\n",
    "\n",
    "For instance, consider how the tokens ‚Äúinterest‚Äù and ‚Äúing‚Äù can be combined to form ‚Äúinteresting‚Äù, or ‚Äúed‚Äù can be appended to form ‚Äúinterested.‚Äù\n",
    "\n",
    "Each LLM has some special tokens specific to the model. The LLM uses these tokens to open and close the structured components of its generation. For example, to indicate the start or end of a sequence, message, or response. Moreover, the input prompts that we pass to the model are also structured with special tokens. The most important of those is the End of sequence token (EOS).\n",
    "\n",
    "The forms of special tokens are highly diverse across model providers.\n",
    "\n",
    "The table below illustrates the diversity of special tokens.\n",
    "\n",
    "| Model       | Provider                       | EOS Token              | Functionality                    |\n",
    "|-------------|--------------------------------|-------------------------|----------------------------------|\n",
    "| GPT-4       | OpenAI                         | `<|endoftext|>`         | End of message text              |\n",
    "| Llama 3     | Meta (Facebook AI Research)    | `<|eot_id|>`            | End of sequence                  |\n",
    "| Deepseek-R1 | DeepSeek                       | `<|end_of_sentence|>`   | End of message text              |\n",
    "| SmolLM2     | Hugging Face                   | `<|im_end|>`            | End of instruction or message    |\n",
    "| Gemma       | Google                         | `<end_of_turn>`         | End of conversation turn         |\n",
    "\n",
    "\n",
    "        We do not expect you to memorize these special tokens, but it is important to appreciate their diversity and the role they play in the text generation of LLMs. If you want to know more about special tokens, you can check out the configuration of the model in its Hub repository. For example, you can find the special tokens of the SmolLM2 model in its tokenizer_config.json.\n",
    "\n",
    "\n",
    "#### Understanding next token prediction.\n",
    "\n",
    "LLMs are said to be autoregressive, meaning that the output from one pass becomes the input for the next one. This loop continues until the model predicts the next token to be the EOS token, at which point the model can stop.\n",
    "\n",
    "In other words, an LLM will decode text until it reaches the EOS. But what happens during a single decoding loop?\n",
    "\n",
    "While the full process can be quite technical for the purpose of learning agents, here‚Äôs a brief overview:\n",
    "\n",
    "- Once the input text is tokenized, the model computes a representation of the sequence that captures information about the meaning and the position of each token in the input sequence.\n",
    "- This representation goes into the model, which outputs scores that rank the likelihood of each token in its vocabulary as being the next one in the sequence.\n",
    "\n",
    "Based on these scores, we have multiple strategies to select the tokens to complete the sentence.\n",
    "\n",
    "- The easiest decoding strategy would be to always take the token with the maximum score.\n",
    "\n",
    "You can interact with the decoding process yourself with SmolLM2 in this Space (remember, it decodes until reaching an EOS token which is <|im_end|> for this model):\n",
    "\n",
    "<img src=\"../assets/Unit1_step2_Example_Tokenization_Decoding.png\" alt=\"Decoding\" width=\"700px\" />\n",
    "\n",
    "- But there are more advanced decoding strategies. For example, beam search explores multiple candidate sequences to find the one with the maximum total score‚Äìeven if some individual tokens have lower scores.\n",
    "    - Beam Search Parameters:\n",
    "        - Sentence to decode from (inputs): the input sequence to your decoder.\n",
    "        - Number of steps (max_new_tokens): the number of tokens to generate.\n",
    "        - Number of beams (num_beams): the number of beams to use.\n",
    "        - Length penalty (length_penalty): the length penalty to apply to outputs. length_penalty > 0.0 promotes longer sequences, while length_penalty < 0.0 encourages shorter sequences. This parameter will not impact the beam search paths, but only influence the choice of sequences in the end towards longer or shorter sequences.\n",
    "        - Number of return sequences (num_return_sequences): the number of sequences to be returned at the end of generation. Should be <= num_beams.\n",
    "\n",
    "\n",
    "#### Attention is all you need\n",
    "\n",
    "A key aspect of the Transformer architecture is Attention. When predicting the next word, not every word in a sentence is equally important; words like ‚ÄúFrance‚Äù and ‚Äúcapital‚Äù in the sentence ‚ÄúThe capital of France is ‚Ä¶‚Äù carry the most meaning.\n",
    "\n",
    "This process of identifying the most relevant words to predict the next token has proven to be incredibly effective.\n",
    "Although the basic principle of LLMs‚Äîpredicting the next token‚Äîhas remained consistent since GPT-2, there have been significant advancements in scaling neural networks and making the attention mechanism work for longer and longer sequences.\n",
    "\n",
    "If you‚Äôve interacted with LLMs, you‚Äôre probably familiar with the term context length, which refers to the maximum number of tokens the LLM can process, and the maximum attention span it has.\n",
    "\n",
    "\n",
    "#### Prompting the LLM is important\n",
    "\n",
    "Considering that the only job of an LLM is to predict the next token by looking at every input token, and to choose which tokens are ‚Äúimportant‚Äù, the wording of your input sequence is very important.\n",
    "\n",
    "The input sequence you provide an LLM is called a prompt. Careful design of the prompt makes it easier to guide the generation of the LLM toward the desired output.\n",
    "\n",
    "\n",
    "#### How are LLMs trained?\n",
    "\n",
    "LLMs are trained on large datasets of text, where they learn to predict the next word in a sequence through a self-supervised or masked language modeling objective.\n",
    "\n",
    "From this unsupervised learning, the model learns the structure of the language and underlying patterns in text, allowing the model to generalize to unseen data.\n",
    "\n",
    "After this initial pre-training, LLMs can be fine-tuned on a supervised learning objective to perform specific tasks. For example, some models are trained for conversational structures or tool usage, while others focus on classification or code generation.\n",
    "\n",
    "\n",
    "#### How can I use LLMs?\n",
    "\n",
    "You have two main options:\n",
    "\n",
    "        1. Run Locally (if you have sufficient hardware).\n",
    "\n",
    "        2. Use a Cloud/API (e.g., via the Hugging Face Serverless Inference API).\n",
    "\n",
    "Throughout this course, we will primarily use models via APIs on the Hugging Face Hub. Later on, we will explore how to run these models locally on your hardware.\n",
    "\n",
    "\n",
    "#### How are LLMs used in AI Agents?\n",
    "\n",
    "LLMs are a key component of AI Agents, providing the foundation for understanding and generating human language.\n",
    "\n",
    "They can interpret user instructions, maintain context in conversations, define a plan and decide which tools to use.\n",
    "\n",
    "We will explore these steps in more detail in this Unit, but for now, what you need to understand is that the LLM is the brain of the Agent.\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "That was a lot of information! We‚Äôve covered the basics of what LLMs are, how they function, and their role in powering AI agents.\n",
    "\n",
    "If you‚Äôd like to dive even deeper into the fascinating world of language models and natural language processing, don‚Äôt hesitate to check out our free [NLP course](https://huggingface.co/learn/llm-course/chapter1/1).\n",
    "\n",
    "Now that we understand how LLMs work, it‚Äôs time to see how LLMs structure their generations in a conversational context.\n",
    "\n",
    "To run [this notebook](https://huggingface.co/agents-course/notebooks/blob/main/dummy_agent_library.ipynb), you need a Hugging Face token that you can get from https://hf.co/settings/tokens.\n",
    "\n",
    "For more information on how to run Jupyter Notebooks, checkout [Jupyter Notebooks on the Hugging Face Hub](https://huggingface.co/docs/hub/notebooks).\n",
    "\n",
    "You also need to request access to the [Meta Llama models](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a05593f",
   "metadata": {},
   "source": [
    "### Step 2: What are Tools?\n",
    "\n",
    "<img src=\"../assets/Unit1_step3.png\" alt=\"tools\" width=\"700px\" />\n",
    "\n",
    "\n",
    "One crucial aspect of AI Agents is their ability to take actions. As we saw, this happens through the use of Tools.\n",
    "\n",
    "In this section, we‚Äôll learn what Tools are, how to design them effectively, and how to integrate them into your Agent via the System Message.\n",
    "\n",
    "By giving your Agent the right Tools‚Äîand clearly describing how those Tools work‚Äîyou can dramatically increase what your AI can accomplish. Let‚Äôs dive in!\n",
    "\n",
    "\n",
    "#### What are AI Tools?\n",
    "A Tool is a function given to the LLM. This function should fulfill a clear objective.\n",
    "\n",
    "Here are some commonly used tools in AI agents:\n",
    "\n",
    "| Tool             | Description                                                   |\n",
    "|------------------|---------------------------------------------------------------|\n",
    "| Web Search       | Allows the agent to fetch up-to-date information from the internet. |\n",
    "| Image Generation | Creates images based on text descriptions.                   |\n",
    "| Retrieval        | Retrieves information from an external source.               |\n",
    "| API Interface    | Interacts with an external API (GitHub, YouTube, Spotify, etc.). |\n",
    "\n",
    "\n",
    "Those are only examples, as you can in fact create a tool for any use case!\n",
    "\n",
    "A good tool should be something that complements the power of an LLM.\n",
    "\n",
    "For instance, if you need to perform arithmetic, giving a calculator tool to your LLM will provide better results than relying on the native capabilities of the model.\n",
    "\n",
    "Furthermore, LLMs predict the completion of a prompt based on their training data, which means that their internal knowledge only includes events prior to their training. Therefore, if your agent needs up-to-date data you must provide it through some tool.\n",
    "\n",
    "For instance, if you ask an LLM directly (without a search tool) for today‚Äôs weather, the LLM will potentially hallucinate random weather.\n",
    "\n",
    "A Tool should contain:\n",
    "\n",
    "- A textual description of what the function does.\n",
    "- A Callable (something to perform an action).\n",
    "- Arguments with typings.\n",
    "- (Optional) Outputs with typings.\n",
    "\n",
    "#### How do tools work?\n",
    "\n",
    "LLMs, as we saw, can only receive text inputs and generate text outputs. They have no way to call tools on their own. When we talk about providing tools to an Agent, we mean teaching the LLM about the existence of these tools and instructing it to generate text-based invocations when needed.\n",
    "\n",
    "For example, if we provide a tool to check the weather at a location from the internet and then ask the LLM about the weather in Paris, the LLM will recognize that this is an opportunity to use the ‚Äúweather‚Äù tool. Instead of retrieving the weather data itself, the LLM will generate text that represents a tool call, such as call weather_tool(‚ÄòParis‚Äô).\n",
    "\n",
    "The Agent then reads this response, identifies that a tool call is required, executes the tool on the LLM‚Äôs behalf, and retrieves the actual weather data.\n",
    "\n",
    "The Tool-calling steps are typically not shown to the user: the Agent appends them as a new message before passing the updated conversation to the LLM again. The LLM then processes this additional context and generates a natural-sounding response for the user. From the user‚Äôs perspective, it appears as if the LLM directly interacted with the tool, but in reality, it was the Agent that handled the entire execution process in the background.\n",
    "\n",
    "We‚Äôll talk a lot more about this process in future sessions.\n",
    "\n",
    "#### How do we give tools to an LLM?\n",
    "\n",
    "The complete answer may seem overwhelming, but we essentially use the system prompt to provide textual descriptions of available tools to the model:\n",
    "\n",
    "```python\n",
    "system_message = \"\"\"You are an AI assistant designed to help users efficiently and accurately. Your primary goal is to provide helpful, precise, and clear responses.\n",
    "\n",
    "You have access to the following tools:\n",
    "{tools_description}\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "For this to work, we have to be very precise and accurate about:\n",
    "\n",
    "1. What the tool does\n",
    "2. What exact inputs it expects\n",
    "\n",
    "This is the reason why tool descriptions are usually provided using expressive but precise structures, such as computer languages or JSON. It‚Äôs not necessary to do it like that, any precise and coherent format would work.\n",
    "\n",
    "If this seems too theoretical, let‚Äôs understand it through a concrete example.\n",
    "\n",
    "We will implement a simplified calculator tool that will just multiply two integers. This could be our Python implementation:\n",
    "\n",
    "```python\n",
    "def calculator(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two integers.\"\"\"\n",
    "    return a * b\n",
    "```\n",
    "\n",
    "So our tool is called calculator, it multiplies two integers, and it requires the following inputs:\n",
    "\n",
    "- a (int): An integer.\n",
    "- b (int): An integer.\n",
    "The output of the tool is another integer number that we can describe like this:\n",
    "\n",
    "(int): The product of a and b.\n",
    "All of these details are important. Let‚Äôs put them together in a text string that describes our tool for the LLM to understand.\n",
    "\n",
    "        Tool Name: calculator, Description: Multiply two integers., Arguments: a: int, b: int, Outputs: int\n",
    "\n",
    "\"Reminder: This textual description is what we want the LLM to know about the tool.\"\n",
    "\n",
    "\n",
    "When we pass the previous string as part of the input to the LLM, the model will recognize it as a tool, and will know what it needs to pass as inputs and what to expect from the output.\n",
    "\n",
    "If we want to provide additional tools, we must be consistent and always use the same format. This process can be fragile, and we might accidentally overlook some details.\n",
    "\n",
    "Is there a better way?\n",
    "\n",
    "Auto-formatting Tool sections\n",
    "Our tool was written in Python, and the implementation already provides everything we need:\n",
    "\n",
    "- A descriptive name of what it does: calculator\n",
    "- A longer description, provided by the function‚Äôs docstring comment: Multiply two integers.\n",
    "- The inputs and their type: the function clearly expects two ints.\n",
    "- The type of the output.\n",
    "\n",
    "There‚Äôs a reason people use programming languages: they are expressive, concise, and precise.\n",
    "\n",
    "We could provide the Python source code as the specification of the tool for the LLM, but the way the tool is implemented does not matter. All that matters is its name, what it does, the inputs it expects and the output it provides.\n",
    "\n",
    "We will leverage Python‚Äôs introspection features to leverage the source code and build a tool description automatically for us. All we need is that the tool implementation uses type hints, docstrings, and sensible function names. We will write some code to extract the relevant portions from the source code.\n",
    "\n",
    "After we are done, we‚Äôll only need to use a Python decorator to indicate that the calculator function is a tool:\n",
    "\n",
    "```python\n",
    "@tool\n",
    "def calculator(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two integers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "print(calculator.to_string())\n",
    "```\n",
    "\n",
    "Note the @tool decorator before the function definition.\n",
    "\n",
    "With the implementation we‚Äôll see next, we will be able to retrieve the following text automatically from the source code via the to_string() function provided by the decorator:\n",
    "\n",
    "        Tool Name: calculator, Description: Multiply two integers., Arguments: a: int, b: int, Outputs: int\n",
    "\n",
    "As you can see, it‚Äôs the same thing we wrote manually before!\n",
    "\n",
    "\n",
    "#### Generic Tool implementation\n",
    "\n",
    "We create a generic Tool class that we can reuse whenever we need to use a tool. This class will be responsible for generating the textual description of the tool, and for calling the function when needed.\n",
    "\n",
    "        \"Disclaimer: This example implementation is fictional but closely resembles real implementations in most libraries.\"\n",
    "\n",
    "\n",
    "```python\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "class Tool:\n",
    "    \"\"\"\n",
    "    A class representing a reusable piece of code (Tool).\n",
    "\n",
    "    Attributes:\n",
    "        name (str): Name of the tool.\n",
    "        description (str): A textual description of what the tool does.\n",
    "        func (callable): The function this tool wraps.\n",
    "        arguments (list): A list of argument.\n",
    "        outputs (str or list): The return type(s) of the wrapped function.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 name: str,\n",
    "                 description: str,\n",
    "                 func: Callable,\n",
    "                 arguments: list,\n",
    "                 outputs: str):\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "        self.func = func\n",
    "        self.arguments = arguments\n",
    "        self.outputs = outputs\n",
    "\n",
    "    def to_string(self) -> str:\n",
    "        \"\"\"\n",
    "        Return a string representation of the tool,\n",
    "        including its name, description, arguments, and outputs.\n",
    "        \"\"\"\n",
    "        args_str = \", \".join([\n",
    "            f\"{arg_name}: {arg_type}\" for arg_name, arg_type in self.arguments\n",
    "        ])\n",
    "\n",
    "        return (\n",
    "            f\"Tool Name: {self.name},\"\n",
    "            f\" Description: {self.description},\"\n",
    "            f\" Arguments: {args_str},\"\n",
    "            f\" Outputs: {self.outputs}\"\n",
    "        )\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Invoke the underlying function (callable) with provided arguments.\n",
    "        \"\"\"\n",
    "        return self.func(*args, **kwargs)\n",
    "```\n",
    "\n",
    "\n",
    "It may seem complicated, but if we go slowly through it we can see what it does. We define a Tool class that includes:\n",
    "\n",
    "- name (str): The name of the tool.\n",
    "- description (str): A brief description of what the tool does.\n",
    "- function (callable): The function the tool executes.\n",
    "- arguments (list): The expected input parameters.\n",
    "- outputs (str or list): The expected outputs of the tool.\n",
    "- __call__(): Calls the function when the tool instance is invoked.\n",
    "- to_string(): Converts the tool‚Äôs attributes into a textual representation.\n",
    "\n",
    "We could create a Tool with this class using code like the following:\n",
    "\n",
    "```python\n",
    "calculator_tool = Tool(\n",
    "    \"calculator\",                   # name\n",
    "    \"Multiply two integers.\",       # description\n",
    "    calculator,                     # function to call\n",
    "    [(\"a\", \"int\"), (\"b\", \"int\")],   # inputs (names and types)\n",
    "    \"int\",                          # output\n",
    ")\n",
    "```\n",
    "\n",
    "But we can also use Python‚Äôs inspect module to retrieve all the information for us! This is what the @tool decorator does.\n",
    "\n",
    "        \"If you are interested, you can disclose the following section to look at the decorator implementation.\"\n",
    "\n",
    "Example of the @tool decorator\n",
    "\n",
    "```python\n",
    "import inspect\n",
    "\n",
    "def tool(func):\n",
    "    \"\"\"\n",
    "    A decorator that creates a Tool instance from the given function.\n",
    "    \"\"\"\n",
    "    # Get the function signature\n",
    "    signature = inspect.signature(func)\n",
    "\n",
    "    # Extract (param_name, param_annotation) pairs for inputs\n",
    "    arguments = []\n",
    "    for param in signature.parameters.values():\n",
    "        annotation_name = (\n",
    "            param.annotation.__name__\n",
    "            if hasattr(param.annotation, '__name__')\n",
    "            else str(param.annotation)\n",
    "        )\n",
    "        arguments.append((param.name, annotation_name))\n",
    "\n",
    "    # Determine the return annotation\n",
    "    return_annotation = signature.return_annotation\n",
    "    if return_annotation is inspect._empty:\n",
    "        outputs = \"No return annotation\"\n",
    "    else:\n",
    "        outputs = (\n",
    "            return_annotation.__name__\n",
    "            if hasattr(return_annotation, '__name__')\n",
    "            else str(return_annotation)\n",
    "        )\n",
    "\n",
    "    # Use the function's docstring as the description (default if None)\n",
    "    description = func.__doc__ or \"No description provided.\"\n",
    "\n",
    "    # The function name becomes the Tool name\n",
    "    name = func.__name__\n",
    "\n",
    "    # Return a new Tool instance\n",
    "    return Tool(\n",
    "        name=name,\n",
    "        description=description,\n",
    "        func=func,\n",
    "        arguments=arguments,\n",
    "        outputs=outputs\n",
    "    )\n",
    "```\n",
    "\n",
    "Just to reiterate, with this decorator in place we can implement our tool like this:\n",
    "\n",
    "```python\n",
    "@tool\n",
    "\n",
    "def calculator(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two integers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "print(calculator.to_string())\n",
    "```\n",
    "\n",
    "And we can use the Tool‚Äôs to_string method to automatically retrieve a text suitable to be used as a tool description for an LLM:\n",
    "\n",
    "        \"Tool Name: calculator, Description: Multiply two integers., Arguments: a: int, b: int, Outputs: int\"\n",
    "\n",
    "\n",
    "The description is injected in the system prompt. Taking the example with which we started this section, here is how it would look like after replacing the tools_description:\n",
    "\n",
    "```python\n",
    "system_message=\"\"\"You are an AI assistant designed to help users efficiently and accurately. Your primary goal is to provide helpful, precise, and clear responses.\n",
    "\n",
    "You have access to the following tools:\n",
    "Tool Name: calculator, Description: Multiply two integers., Arguments: a: int, b: int, Outputs: int\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "In the Actions section, we will learn more about how an Agent can Call this tool we just created.\n",
    "\n",
    "Model Context Protocol (MCP): a unified tool interface\n",
    "Model Context Protocol (MCP) is an open protocol that standardizes how applications provide tools to LLMs. MCP provides:\n",
    "\n",
    "- A growing list of pre-built integrations that your LLM can directly plug into\n",
    "- The flexibility to switch between LLM providers and vendors\n",
    "- Best practices for securing your data within your infrastructure\n",
    "\n",
    "This means that any framework implementing MCP can leverage tools defined within the protocol, eliminating the need to reimplement the same tool interface for each framework.\n",
    "\n",
    "----\n",
    "\n",
    "Tools play a crucial role in enhancing the capabilities of AI agents.\n",
    "\n",
    "To summarize, we learned:\n",
    "\n",
    "- What Tools Are: Functions that give LLMs extra capabilities, such as performing calculations or accessing external data.\n",
    "\n",
    "- How to Define a Tool: By providing a clear textual description, inputs, outputs, and a callable function.\n",
    "\n",
    "- Why Tools Are Essential: They enable Agents to overcome the limitations of static model training, handle real-time tasks, and perform specialized actions.\n",
    "\n",
    "Now, we can move on to the Agent Workflow where you‚Äôll see how an Agent observes, thinks, and acts. This brings together everything we‚Äôve covered so far and sets the stage for creating your own fully functional AI Agent.\n",
    "\n",
    "But first, it‚Äôs time for another short quiz!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75504a37",
   "metadata": {},
   "source": [
    "### Step 4: Agent Workflow\n",
    "\n",
    "<img src=\"../assets/Unit1_step4.png\" alt=\"Agent Workflow\" width=\"500px\" />\n",
    "\n",
    "#### Understanding AI Agents through the Thought-Action-Observation Cycle\n",
    "\n",
    "In the previous sections, we learned:\n",
    "\n",
    "- How tools are made available to the agent in the system prompt.\n",
    "- How AI agents are systems that can ‚Äòreason‚Äô, plan, and interact with their environment.\n",
    "\n",
    "In this section, we‚Äôll explore the complete AI Agent Workflow, a cycle we defined as Thought-Action-Observation.\n",
    "\n",
    "And then, we‚Äôll dive deeper on each of these steps.\n",
    "\n",
    "\n",
    "##### The Core Components\n",
    "\n",
    "Agents work in a continuous cycle of: thinking (Thought) ‚Üí acting (Act) and observing (Observe).\n",
    "\n",
    "Let‚Äôs break down these actions together:\n",
    "\n",
    "1. Thought: The LLM part of the Agent decides what the next step should be.\n",
    "2. Action: The agent takes an action, by calling the tools with the associated arguments.\n",
    "3. Observation: The model reflects on the response from the tool.\n",
    "\n",
    "\n",
    "##### The Thought-Action-Observation Cycle\n",
    "\n",
    "The three components work together in a continuous loop. To use an analogy from programming, the agent uses a while loop: the loop continues until the objective of the agent has been fulfilled.\n",
    "\n",
    "<img src=\"../assets/Unit1_step4_TAO_cycle.png\" alt=\"Agent Workflow\" width=\"500px\" />\n",
    "\n",
    "In many Agent frameworks, the rules and guidelines are embedded directly into the system prompt, ensuring that every cycle adheres to a defined logic.\n",
    "\n",
    "In a simplified version, our system prompt may look like this:\n",
    "\n",
    "<img src=\"../assets/Unit1_step4_prompt_example.png\" alt=\"System Prompt\" width=\"500px\" />\n",
    "\n",
    "We see here that in the System Message we defined :\n",
    "\n",
    "- The Agent‚Äôs behavior.\n",
    "- The Tools our Agent has access to, as we described in the previous section.\n",
    "- The Thought-Action-Observation Cycle, that we bake into the LLM instructions.\n",
    "\n",
    "##### Example: Alfred the Weather Agent\n",
    "\n",
    "A user asks Alfred: ‚ÄúWhat‚Äôs the current weather in New York?‚Äù\n",
    "\n",
    "Alfred‚Äôs job is to answer this query using a weather API tool.\n",
    "\n",
    "Here‚Äôs how the cycle unfolds:\n",
    "\n",
    "1. Thought\n",
    "\n",
    "    - Internal Reasoning:\n",
    "\n",
    "    Upon receiving the query, Alfred‚Äôs internal dialogue might be:\n",
    "\n",
    "    ‚ÄúThe user needs current weather information for New York. I have access to a tool that fetches weather data. First, I need to call the weather API to get up-to-date details.‚Äù\n",
    "\n",
    "    This step shows the agent breaking the problem into steps: first, gathering the necessary data.\n",
    "\n",
    "\n",
    "\n",
    "2. Action\n",
    "\n",
    "    - Tool Usage:\n",
    "\n",
    "    Based on its reasoning and the fact that Alfred knows about a get_weather tool, Alfred prepares a JSON-formatted command that calls the weather API tool. For example, its first action could be:\n",
    "\n",
    "    Thought: I need to check the current weather for New York.\n",
    "\n",
    "    ```json\n",
    "       {\n",
    "         \"action\": \"get_weather\",\n",
    "         \"action_input\": {\n",
    "           \"location\": \"New York\"\n",
    "         }\n",
    "       }\n",
    "    ```\n",
    "    \n",
    "    Here, the action clearly specifies which tool to call (e.g., get_weather) and what parameter to pass (the ‚Äúlocation‚Äù: ‚ÄúNew York‚Äù).\n",
    "\n",
    "3. Observation\n",
    "\n",
    "    - Feedback from the Environment:\n",
    "\n",
    "    After the tool call, Alfred receives an observation. This might be the raw weather data from the API such as:\n",
    "\n",
    "    ‚ÄúCurrent weather in New York: partly cloudy, 15¬∞C, 60% humidity.‚Äù\n",
    "\n",
    "\n",
    "    This observation is then added to the prompt as additional context. It functions as real-world feedback, confirming whether the action succeeded and providing the needed details.\n",
    "\n",
    "4. Updated thought\n",
    "\n",
    "    - Reflecting:\n",
    "\n",
    "    With the observation in hand, Alfred updates its internal reasoning:\n",
    "\n",
    "    ‚ÄúNow that I have the weather data for New York, I can compile an answer for the user.‚Äù\n",
    "\n",
    "\n",
    "5. Final Action\n",
    "\n",
    "    Alfred then generates a final response formatted as we told it to:\n",
    "\n",
    "    - Thought: I have the weather data now. The current weather in New York is partly cloudy with a temperature of 15¬∞C and 60% humidity.‚Äù\n",
    "\n",
    "    - Final answer : The current weather in New York is partly cloudy with a temperature of 15¬∞C and 60% humidity.\n",
    "\n",
    "    This final action sends the answer back to the user, closing the loop.\n",
    "\n",
    "\n",
    "\n",
    "What we see in this example:\n",
    "\n",
    "- Agents iterate through a loop until the objective is fulfilled:\n",
    "\n",
    "    Alfred‚Äôs process is cyclical. It starts with a thought, then acts by calling a tool, and finally observes the outcome. If the observation had indicated an error or incomplete data, Alfred could have re-entered the cycle to correct its approach.\n",
    "\n",
    "- Tool Integration:\n",
    "\n",
    "    The ability to call a tool (like a weather API) enables Alfred to go beyond static knowledge and retrieve real-time data, an essential aspect of many AI Agents.\n",
    "\n",
    "- Dynamic Adaptation:\n",
    "\n",
    "    Each cycle allows the agent to incorporate fresh information (observations) into its reasoning (thought), ensuring that the final answer is well-informed and accurate.\n",
    "\n",
    "This example showcases the core concept behind the ReAct cycle (a concept we‚Äôre going to develop in the next section): the interplay of Thought, Action, and Observation empowers AI agents to solve complex tasks iteratively.\n",
    "\n",
    "By understanding and applying these principles, you can design agents that not only reason about their tasks but also effectively utilize external tools to complete them, all while continuously refining their output based on environmental feedback.\n",
    "\n",
    "---\n",
    "\n",
    "Let‚Äôs now dive deeper into the Thought, Action, Observation as the individual steps of the process.\n",
    "\n",
    "\n",
    "##### Thought: Internal Reasoning and the ReAct Approach\n",
    "\n",
    "        In this section, we dive into the inner workings of an AI agent‚Äîits ability to reason and plan. We‚Äôll explore how the agent leverages its internal dialogue to analyze information, break down complex problems into manageable steps, and decide what action to take next. Additionally, we introduce the ReAct approach, a prompting technique that encourages the model to think ‚Äústep by step‚Äù before acting.\n",
    "\n",
    "Thoughts represent the Agent‚Äôs internal reasoning and planning processes to solve the task.\n",
    "\n",
    "This utilises the agent‚Äôs Large Language Model (LLM) capacity to analyze information when presented in its prompt.\n",
    "\n",
    "Think of it as the agent‚Äôs internal dialogue, where it considers the task at hand and strategizes its approach.\n",
    "\n",
    "The Agent‚Äôs thoughts are responsible for accessing current observations and decide what the next action(s) should be.\n",
    "\n",
    "Through this process, the agent can break down complex problems into smaller, more manageable steps, reflect on past experiences, and continuously adjust its plans based on new information.\n",
    "\n",
    "        \"Note: In the case of LLMs fine-tuned for function-calling, the thought process is optional. In case you‚Äôre not familiar with function-calling, there will be more details in the Actions section.\"\n",
    "\n",
    "Here are some examples of common thoughts:\n",
    "\n",
    "| Type of Thought     | Example                                                                                                    |\n",
    "|---------------------|------------------------------------------------------------------------------------------------------------|\n",
    "| Planning            | ‚ÄúI need to break this task into three steps: 1) gather data, 2) analyze trends, 3) generate report‚Äù         |\n",
    "| Analysis            | ‚ÄúBased on the error message, the issue appears to be with the database connection parameters‚Äù              |\n",
    "| Decision Making     | ‚ÄúGiven the user‚Äôs budget constraints, I should recommend the mid-tier option‚Äù                              |\n",
    "| Problem Solving     | ‚ÄúTo optimize this code, I should first profile it to identify bottlenecks‚Äù                                 |\n",
    "| Memory Integration  | ‚ÄúThe user mentioned their preference for Python earlier, so I‚Äôll provide examples in Python‚Äù               |\n",
    "| Self-Reflection     | ‚ÄúMy last approach didn‚Äôt work well, I should try a different strategy‚Äù                                      |\n",
    "| Goal Setting        | ‚ÄúTo complete this task, I need to first establish the acceptance criteria‚Äù                                 |\n",
    "| Prioritization      | ‚ÄúThe security vulnerability should be addressed before adding new features‚Äù                                |\n",
    "\n",
    "\n",
    "##### The ReAct Approach\n",
    "\n",
    "A key method is the ReAct approach, which is the concatenation of ‚ÄúReasoning‚Äù (Think) with ‚ÄúActing‚Äù (Act).\n",
    "\n",
    "ReAct is a simple prompting technique that appends ‚ÄúLet‚Äôs think step by step‚Äù before letting the LLM decode the next tokens.\n",
    "\n",
    "Indeed, prompting the model to think ‚Äústep by step‚Äù encourages the decoding process toward next tokens that generate a plan, rather than a final solution, since the model is encouraged to decompose the problem into sub-tasks.\n",
    "\n",
    "This allows the model to consider sub-steps in more detail, which in general leads to less errors than trying to generate the final solution directly.\n",
    "\n",
    "        We have recently seen a lot of interest for reasoning strategies. This is what's behind models like Deepseek R1 or OpenAI's o1, which have been fine-tuned to \"think before answering\".\n",
    "        These models have been trained to always include specific thinking sections (enclosed between <think> and </think> special tokens). This is not just a prompting technique like ReAct, but a training method where the model learns to generate these sections after analyzing thousands of examples that show what we expect it to do.\n",
    "\n",
    "\n",
    "\n",
    "#### Actions: Enabling the Agent to Engage with Its Environment\n",
    "\n",
    "        In this section, we explore the concrete steps an AI agent takes to interact with its environment.\n",
    "        We‚Äôll cover how actions are represented (using JSON or code), the importance of the stop and parse approach, and introduce different types of agents.\n",
    "\n",
    "Actions are the concrete steps an AI agent takes to interact with its environment.\n",
    "\n",
    "Whether it‚Äôs browsing the web for information or controlling a physical device, each action is a deliberate operation executed by the agent.\n",
    "\n",
    "For example, an agent assisting with customer service might retrieve customer data, offer support articles, or transfer issues to a human representative.\n",
    "\n",
    "##### Types of Agent Actions\n",
    "\n",
    "There are multiple types of Agents that take actions differently:\n",
    "\n",
    "| Type of Agent          | Description                                                                                      |\n",
    "|------------------------|--------------------------------------------------------------------------------------------------|\n",
    "| JSON Agent             | The Action to take is specified in JSON format.                                                  |\n",
    "| Code Agent             | The Agent writes a code block that is interpreted externally.                                    |\n",
    "| Function-calling Agent | It is a subcategory of the JSON Agent which has been fine-tuned to generate a new message for each action. |\n",
    "\n",
    "\n",
    "Actions themselves can serve many purposes:\n",
    "\n",
    "| Type of Action         | Description                                                                                      |\n",
    "|------------------------|--------------------------------------------------------------------------------------------------|\n",
    "| Information Gathering  | Performing web searches, querying databases, or retrieving documents.                            |\n",
    "| Tool Usage             | Making API calls, running calculations, and executing code.                                      |\n",
    "| Environment Interaction| Manipulating digital interfaces or controlling physical devices.                                |\n",
    "| Communication          | Engaging with users via chat or collaborating with other agents.                                 |\n",
    "\n",
    "\n",
    "The LLM only handles text and uses it to describe the action it wants to take and the parameters to supply to the tool. For an agent to work properly, the LLM must STOP generating new tokens after emitting all the tokens to define a complete Action. This passes control from the LLM back to the agent and ensures the result is parseable - whether the intended format is JSON, code, or function-calling.\n",
    "\n",
    "\n",
    "##### The Stop and Parse Approach\n",
    "\n",
    "One key method for implementing actions is the stop and parse approach. This method ensures that the agent‚Äôs output is structured and predictable:\n",
    "\n",
    "1. Generation in a Structured Format:\n",
    "\n",
    "    The agent outputs its intended action in a clear, predetermined format (JSON or code).\n",
    "\n",
    "2. Halting Further Generation:\n",
    "\n",
    "    Once the text defining the action has been emitted, the LLM stops generating additional tokens. This prevents extra or erroneous output.\n",
    "\n",
    "3. Parsing the Output:\n",
    "\n",
    "    An external parser reads the formatted action, determines which Tool to call, and extracts the required parameters.\n",
    "\n",
    "For example, an agent needing to check the weather might output:\n",
    "\n",
    "```json\n",
    "Thought: I need to check the current weather for New York.\n",
    "Action :\n",
    "{\n",
    "  \"action\": \"get_weather\",\n",
    "  \"action_input\": {\"location\": \"New York\"}\n",
    "}\n",
    "```\n",
    "\n",
    "The framework can then easily parse the name of the function to call and the arguments to apply.\n",
    "\n",
    "This clear, machine-readable format minimizes errors and enables external tools to accurately process the agent‚Äôs command.\n",
    "\n",
    "Note: Function-calling agents operate similarly by structuring each action so that a designated function is invoked with the correct arguments. We‚Äôll dive deeper into those types of Agents in a future Unit.\n",
    "\n",
    "##### Code Agents\n",
    "\n",
    "An alternative approach is using Code Agents. The idea is: instead of outputting a simple JSON object, a Code Agent generates an executable code block‚Äîtypically in a high-level language like Python.\n",
    "\n",
    "<img src=\"../assets/Unit1_step4_Code_Agents.png\" alt=\"Code Agent\" width=\"800px\" />\n",
    "\n",
    "This approach offers several advantages:\n",
    "\n",
    "- Expressiveness: Code can naturally represent complex logic, including loops, conditionals, and nested functions, providing greater flexibility than JSON.\n",
    "- Modularity and Reusability: Generated code can include functions and modules that are reusable across different actions or tasks.\n",
    "- Enhanced Debuggability: With a well-defined programming syntax, code errors are often easier to detect and correct.\n",
    "- Direct Integration: Code Agents can integrate directly with external libraries and APIs, enabling more complex operations such as data processing or real-time decision making.\n",
    "\n",
    "For example, a Code Agent tasked with fetching the weather might generate the following Python snippet:\n",
    "\n",
    "```python\n",
    "# Code Agent Example: Retrieve Weather Information\n",
    "def get_weather(city):\n",
    "    import requests\n",
    "    api_url = f\"https://api.weather.com/v1/location/{city}?apiKey=YOUR_API_KEY\"\n",
    "    response = requests.get(api_url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data.get(\"weather\", \"No weather information available\")\n",
    "    else:\n",
    "        return \"Error: Unable to fetch weather data.\"\n",
    "\n",
    "# Execute the function and prepare the final answer\n",
    "result = get_weather(\"New York\")\n",
    "final_answer = f\"The current weather in New York is: {result}\"\n",
    "print(final_answer)\n",
    "```\n",
    "\n",
    "In this example, the Code Agent:\n",
    "\n",
    "Retrieves weather data via an API call,\n",
    "Processes the response,\n",
    "And uses the print() function to output a final answer.\n",
    "This method also follows the stop and parse approach by clearly delimiting the code block and signaling when execution is complete (here, by printing the final_answer).\n",
    "\n",
    "---\n",
    "\n",
    "We learned that Actions bridge an agent‚Äôs internal reasoning and its real-world interactions by executing clear, structured tasks‚Äîwhether through JSON, code, or function calls.\n",
    "\n",
    "This deliberate execution ensures that each action is precise and ready for external processing via the stop and parse approach. In the next section, we will explore Observations to see how agents capture and integrate feedback from their environment.\n",
    "\n",
    "After this, we will finally be ready to build our first Agent!\n",
    "\n",
    "\n",
    "\n",
    "##### Observe: Integrating Feedback to Reflect and Adapt\n",
    "\n",
    "Observations are how an Agent perceives the consequences of its actions.\n",
    "\n",
    "They provide crucial information that fuels the Agent‚Äôs thought process and guides future actions.\n",
    "\n",
    "They are signals from the environment‚Äîwhether it‚Äôs data from an API, error messages, or system logs‚Äîthat guide the next cycle of thought.\n",
    "\n",
    "In the observation phase, the agent:\n",
    "\n",
    "- Collects Feedback: Receives data or confirmation that its action was successful (or not).\n",
    "- Appends Results: Integrates the new information into its existing context, effectively updating its memory.\n",
    "- Adapts its Strategy: Uses this updated context to refine subsequent thoughts and actions.\n",
    "\n",
    "For example, if a weather API returns the data ‚Äúpartly cloudy, 15¬∞C, 60% humidity‚Äù, this observation is appended to the agent‚Äôs memory (at the end of the prompt).\n",
    "\n",
    "The Agent then uses it to decide whether additional information is needed or if it‚Äôs ready to provide a final answer.\n",
    "\n",
    "This iterative incorporation of feedback ensures the agent remains dynamically aligned with its goals, constantly learning and adjusting based on real-world outcomes.\n",
    "\n",
    "These observations can take many forms, from reading webpage text to monitoring a robot arm‚Äôs position. This can be seen like Tool ‚Äúlogs‚Äù that provide textual feedback of the Action execution.\n",
    "\n",
    "| Type of Observation   | Example                                                                    |\n",
    "|------------------------|----------------------------------------------------------------------------|\n",
    "| System Feedback        | Error messages, success notifications, status codes                       |\n",
    "| Data Changes           | Database updates, file system modifications, state changes                |\n",
    "| Environmental Data     | Sensor readings, system metrics, resource usage                           |\n",
    "| Response Analysis      | API responses, query results, computation outputs                         |\n",
    "| Time-based Events      | Deadlines reached, scheduled tasks completed                              |\n",
    "\n",
    "\n",
    "##### How Are the Results Appended?\n",
    "\n",
    "After performing an action, the framework follows these steps in order:\n",
    "\n",
    "1. Parse the action to identify the function(s) to call and the argument(s) to use.\n",
    "2. Execute the action.\n",
    "3. Append the result as an Observation.\n",
    "\n",
    "We‚Äôve now learned the Agent‚Äôs Thought-Action-Observation Cycle.\n",
    "\n",
    "If some aspects still seem a bit blurry, don‚Äôt worry‚Äîwe‚Äôll revisit and deepen these concepts in future Units.\n",
    "\n",
    "Now, it‚Äôs time to put your knowledge into practice by coding your very first Agent!\n",
    "\n",
    "The code for the first agent can be found in the folder [`src/smolagent_basic`]() in the course repository.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
